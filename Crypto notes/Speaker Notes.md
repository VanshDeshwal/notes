ğŸ”¹ Packet Analysis for Network Forensics: A Comprehensive Survey

Slide 1
â€œAlright, so the first paper is a survey on packet analysis in network forensics.
Basically, it looks at how we capture packets, how we analyze them, and even the legal and privacy issues involved.
It doesnâ€™t propose a new system, but instead maps out the existing tools and directions for future research.â€

Slide 2
â€œHere, the tools are grouped by what they focus on.
Some analyze raw packets, others flows, and others the payload.
They also point out AI and machine learning examples â€” like an SVM plugin for Snort, and deep packet inspection for encrypted traffic.
Thereâ€™s also this knowledge-based reasoning idea, using ontologies like PACO and PAO.
Uh, broadly, you can think of two categories: carving and reconstruction tools, and tracing analyzers.â€

Slide 3
â€œSo yeah, this survey is very complete â€” it ties together AI, DPI, and forensics.
But the limitation is that it doesnâ€™t include datasets, metrics, or detailed features.
For future work, they mention privacy-preserving DPI, applying ML to encrypted or unknown traffic, and scaling up for IoT and cloud.
So, itâ€™s more like a roadmap than an experimental paper.â€

ğŸ”¹ Network Traffic Classification for Data Fusion: A Survey

Slide 1
â€œOkay, the second survey is about traffic classification, and how it supports security and malware detection.
Itâ€™s also more of a reference work â€” summarizing techniques, datasets, features, evaluation methods, and open challenges.â€

Slide 2
â€œThey divide methods into five types: statistics-based, correlation-based, behavior-based, payload-based, and port-based.
They review 36 features â€” across packet, flow, and connection levels â€” and they show how combining levels improves detection.
They also look at 16 datasets, like ISCX, KDD Cup99, DARPA.
And they suggest standard criteria to compare methods, like accuracy, robustness, online capability, handling unknown traffic, and granularity.â€

Slide 3
â€œOne strength is how structured it is â€” very useful for newcomers.
But it doesnâ€™t rank which features matter most, and datasets arenâ€™t always clear about OS details or malware versus benign splits.
The main open problems are: reclassifying unknown traffic, making classifiers lightweight but still accurate, and better ground-truth collection.
So again, itâ€™s a structured reference, not a system.â€

ğŸ”¹ NetML: A Challenge for Network Traffic Analytics

Slide 1
â€œAlright, next is the NetML paper, from 2020 at the NetAI Workshop.
The motivation is clear: in computer vision we have ImageNet, in NLP we have COCO, but in network traffic analysis, there wasnâ€™t a benchmark dataset.
So the authors created three open datasets and provided baseline ML results, calling it the NetML challenge.â€

Slide 2 â€” Dataset
â€œThe three datasets target malware detection and traffic classification.
First, the NetML dataset â€” half a million flows, with 20 malware families plus benign traffic. Mostly from Windows, but also some benign Linux and Kali captures.
Second, CICIDS2017 â€” about 550k flows, covering 7 attacks plus benign. Two attacks were excluded because features couldnâ€™t be extracted. This one is mainly Windows.
Third, non-VPN2016 â€” smaller, about 160k flows, focused on classifying applications like chat, email, video, down to fine-grained classes like Skype audio vs Skype chat.
So overall, Windows dominates, but Linux and Kali are also present.â€

Slide 3 â€” Features
â€œThey extracted four categories of features.
Metadata â€” always available, like packet counts, byte counts, flow duration.
Then protocol-specific ones: TLS, DNS, HTTP. For example, TLS cipher suites, DNS queries and answers, HTTP request methods and content types.
They noticed some features stand out â€” like header-bin-40 being smaller in malware flows, or HTTP content type being a strong signal.
But for the baselines, only metadata was used, because not every flow has TLS or HTTP.â€

Slide 4 â€” Method
â€œThey tested three baseline models â€” Random Forest, SVM, and an MLP.
Each represents a different family: ensemble, kernel-based, and neural nets.
Preprocessing included expanding arrays, masking IPs, and standardizing features.
And again, only metadata features were used to keep things consistent.â€

Slide 5 â€” Evaluation
â€œFor binary malware detection, results were excellent â€” Random Forest hit a true positive rate of 99.3% with false alarms under 1%.
For multi-class, CICIDS2017 performed very well, F1 around 0.99.
On NetML dataset, F1 dropped to 0.74 â€” some malware families got confused.
And on non-VPN2016, all models struggled â€” F1 below 0.63, due to class imbalance, like Skype and Facebook being overrepresented.
So in short â€” Random Forest is strong for binary detection, but imbalance is a big challenge for multi-class tasks.â€

ğŸ”¹ MalDIST: From Encrypted Traffic Classification to Malware Detection

Slide 1
â€œOkay, next up is MalDIST, presented at IEEE CCNC 2022.
The idea is pretty neat: instead of making a whole new model, they adapted an existing encrypted traffic classifier, DISTILLER, to detect malware.
They tested it for both binary detection and multi-class classification.â€

Slide 2 â€” Dataset
â€œThe dataset combines benign traffic from StratosphereIPS and ISCX2016, and malicious traffic from Malware-Traffic-Analysis.net â€” focusing on Dridex, Hancitor, Emotet, and Valak.
After filtering, they had around 18,000 sessions, balanced between benign and malicious, and about 58% TLS-encrypted.
Also, since StratosphereIPS benign captures are the same as NetML, we can infer both Windows and Kali traffic are included.â€

Slide 3 â€” Features
â€œThey used three modalities.
First, payload bytes â€” the first 784 bytes of each session.
Second, protocol fields â€” from the first 32 packets, like direction, size, inter-arrival time.
Third, statistical features â€” also from the first 32 packets, grouped into 5 sets, with 14 features each, like min, max, mean, skewness.
So, across layers 2 to 4, they cover raw payload, packet-level details, and compact statistics.â€

Slide 4 â€” Method
â€œThe architecture is multi-modal.
Payload bytes go through 1D CNNs.
Protocol fields go through a BiGRU.
Statistical features go through a BiLSTM, then 2D CNNs.
Outputs are merged, then split into two heads â€” one for binary detection, one for malware family classification.â€

Slide 5 â€” Evaluation
â€œFor binary detection, results were almost perfect â€” 99.7% across accuracy, precision, recall, F1.
For multi-class, MalDIST also outperformed both ML and DL baselines.
Only Dridex was weaker, about 82%, probably due to fewer samples.
But overall, MalDIST set a new benchmark.â€

ğŸ”¹ Unknown Malware Detection Using Network Traffic Classification

Slide 1
â€œNow, this paper is from 2015, IEEE CNS.
It proposes a supervised ML intrusion detection system that works across multiple layers and protocols, unlike rule-based IDS like Snort or Suricata.
They donâ€™t say which OS they used, but since they relied on tcpdump and Wireshark â€” both Linux-native tools â€” itâ€™s safe to assume Linux.â€

Slide 2 â€” Dataset
â€œMalicious traffic came from Verint sandbox, VirusTotal, Emerging Threats, and community datasets.
Benign traffic came from student lab activity and corporate networks.
So, a mix of sandbox and real-world data â€” which is good for testing robustness.â€

Slide 3 â€” Features
â€œThey mapped features to OSI layers.
At transport, things like number of RST, ACK, duplicate ACK, keep-alive packets.
At application, HTTP timing, and DNS features like query rank and number of records.
And cross-layer features like number of flows per window.
So, quite a rich feature set.â€

Slide 4 â€” Method
â€œThey tested NaÃ¯ve Bayes, J48 decision trees, and Random Forest in Weka.
They also did feature selection â€” reduced about a thousand features down to 12 key ones.
So, itâ€™s efficient while still being accurate.â€

Slide 5 â€” Evaluation
â€œRandom Forest was the best.
For family classification, accuracy was almost perfect.
For unseen families, AUC was about 0.98, except Conficker, which was 0.77.
The system also detected threats earlier than Snort or Suricata â€” up to a month before rules were deployed.
Thatâ€™s a strong result, showing generalization beyond signatures.â€

ğŸ”¹ APT Attack Detection Using Flow Network Analysis with Deep Learning

Slide 1
â€œNext, we have a 2021 paper on APT detection at the flow level.
The goal was to classify IPs as normal or infected with APT.â€

Slide 2 â€” Dataset
â€œThey used benign traffic from a Vietnam e-government server, and malicious traffic from the CTU-13 Malware Capture Facility.
CTU-13 includes many malware families, making it realistic.
The dataset is imbalanced â€” way more benign IPs than malicious â€” which matches real-world conditions.â€

Slide 3 â€” Features
â€œAll features came from CICFlowMeter, so flow-level only, no payload.
Layer 2 had counts, Layer 3 overall flow behavior, and Layer 4 timing and TCP connection dynamics.
So itâ€™s metadata-focused and privacy-friendly.â€

Slide 4 â€” Method
â€œThey tried three supervised models: MLP, GCN, and a BiLSTM-GCN hybrid.
The hybrid uses both sequential and graph structure, which helps capture relations between IPs.â€

Slide 5 â€” Evaluation
â€œThe BiLSTM-GCN was the best â€” 99% accuracy and recall, even with imbalance.
MLP and GCN were also good, but slightly weaker.
The key point is: using graph relationships reduced missed detections.
No OS or system context was discussed in the paper.â€

ğŸ”¹ Network Malware Classification: DPI vs Flow Features

Slide 1
â€œAnd finally, a 2015 paper comparing deep packet inspection with flow-based features.
Malware was executed in Windows XP environments, while benign traffic came from home, lab, corporate, and ISP networks.
So, both packet-level and flow-level views were tested.â€

Slide 2
â€œThe malware dataset was manually labeled by family names, using MARFPCAT meta files.
Benign traffic came from diverse environments.
Annotation was manual, which they note as an important dependency.â€

Slide 3
â€œ
In terms of layers: L1â€“L3 were headers, L4 involved payload inspection.â€
Flow-based features came from packet headers: duration, packet counts, byte counts, inter-arrival times.
DPI features were signal-based: FFT coefficients, LPC parameters, MinMax amplitudes, bi-grams.
So flow ignores payload, DPI processes it.

Slide 4
â€œThe flow approach hit at least 98% accuracy across benign datasets, with very low false positives.
DPI also had high accuracy, but struggled with generic malware families because labeling was harder.
The main takeaway is: DPI can be effective with just a couple packets â€” good for early detection â€” while flow-based methods work well when you have more complete statistics.â€